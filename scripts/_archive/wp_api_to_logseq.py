import requests
import os
import re
import html2text
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import hashlib

# --- é…ç½®å€ ---
SITE_URL = "https://uncle6.me"
KB_DIR = "./KB"
OUTPUT_DIR = os.path.join(KB_DIR, "pages")
ASSETS_DIR = os.path.join(KB_DIR, "assets")
PER_PAGE = 20  # æ¯æ¬¡æŠ“å–å¹¾ç¯‡

def ensure_dirs():
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    if not os.path.exists(ASSETS_DIR):
        os.makedirs(ASSETS_DIR)

def download_image(img_url):
    """ä¸‹è¼‰åœ–ç‰‡ä¸¦å­˜å…¥ assetsï¼Œå›å‚³ç›¸å°è·¯å¾‘åç¨± (ä¾‹å¦‚ ../assets/abc.jpg)"""
    try:
        # ç°¡å–®éæ¿¾æ‰é http é–‹é ­çš„ (ä¾‹å¦‚ data:image)
        if not img_url.startswith('http'):
            return img_url

        # ç”¢ç”Ÿå”¯ä¸€æª”åï¼Œé¿å…é‡è¤‡æˆ–éé•·
        # ä½¿ç”¨ MD5 hash ç¢ºä¿åŒä¸€å€‹ç¶²å€çš„åœ–ç‰‡å­˜æˆåŒä¸€ä»½
        file_ext = os.path.splitext(urlparse(img_url).path)[1]
        if not file_ext or len(file_ext) > 5:
            file_ext = ".jpg" # é è¨­ fallback
            
        hash_name = hashlib.md5(img_url.encode('utf-8')).hexdigest()
        filename = f"{hash_name}{file_ext}"
        filepath = os.path.join(ASSETS_DIR, filename)

        # å¦‚æœæª”æ¡ˆå·²ç¶“å­˜åœ¨ï¼Œå°±ä¸ç”¨é‡æŠ“
        if not os.path.exists(filepath):
            # å½è£ User-Agent é¿å…è¢«æ“‹
            headers = {'User-Agent': 'Mozilla/5.0'}
            r = requests.get(img_url, headers=headers, stream=True, timeout=10)
            if r.status_code == 200:
                with open(filepath, 'wb') as f:
                    for chunk in r.iter_content(1024):
                        f.write(chunk)
            else:
                print(f"  âš ï¸  åœ–ç‰‡ä¸‹è¼‰å¤±æ•— (Status {r.status_code}): {img_url}")
                return img_url # ä¸‹è¼‰å¤±æ•—å‰‡ç¶­æŒåŸç¶²å€
        
        # ä½¿ç”¨ç›¸å°è·¯å¾‘ ../assets/ï¼Œé…åˆåˆ†é¡å­è³‡æ–™å¤¾çµæ§‹
        return f"../assets/{filename}"

    except Exception as e:
        print(f"  âš ï¸  åœ–ç‰‡ä¸‹è¼‰éŒ¯èª¤: {e} - {img_url}")
        return img_url

def process_html_images(html_content):
    """ä½¿ç”¨ BeautifulSoup è§£æ HTMLï¼Œä¸‹è¼‰åœ–ç‰‡ä¸¦æ›¿æ› src"""
    if not html_content:
        return ""
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æ‰¾å°‹æ‰€æœ‰åœ–ç‰‡æ¨™ç±¤
    imgs = soup.find_all('img')
    if imgs:
        print(f"  Found {len(imgs)} images, downloading...")
        for img in imgs:
            src = img.get('src')
            if src:
                new_src = download_image(src)
                img['src'] = new_src
                # ç§»é™¤ srcset é¿å… Logseq/ç€è¦½å™¨å„ªå…ˆä½¿ç”¨èˆŠçš„ CDN é€£çµ
                if img.has_attr('srcset'):
                    del img['srcset']
    
    return str(soup)

def convert_to_outliner(html_content):
    # 1. å…ˆè™•ç†åœ–ç‰‡ä¸‹è¼‰
    processed_html = process_html_images(html_content)

    # 2. è½‰ç‚º Markdown
    h = html2text.HTML2Text()
    h.ignore_links = False
    h.body_width = 0 
    h.ignore_images = False 
    markdown = h.handle(processed_html)
    
    # 3. è§£æä¸¦è½‰ç‚º Logseq Outliner æ ¼å¼
    lines = markdown.split('\n')
    
    parsed_items = [] # å„²å­˜ {type, content, level, original_is_list, ...}
    min_header_level = 999
    
    # é è™•ç†ï¼šè§£ææ¯ä¸€è¡Œçš„é¡å‹
    i = 0
    while i < len(lines):
        line = lines[i]
        
        # éæ¿¾é›œè¨Š
        if line == "æœ¬æ–‡ç›®éŒ„" or line == "Toggle" or line == "Table of Contents": 
            i += 1
            continue
            
        # æ›´åŠ å¯¬é¬†çš„ TOC é€£çµåˆ¤æ–· (æ”¯æ´ * æˆ– - é–‹é ­ï¼Œä¸”å…è¨±ç¸®æ’)
        stripped_line = line.strip()
        if (stripped_line.startswith("- [") or stripped_line.startswith("* [")) and "](" in line and "#" in line and "uncle6.me" in line:
            i += 1
            continue

        # ç§»é™¤ Footer ("- è®“æˆ‘å€‘ä¿æŒè¯ç¹«" ä¹‹å¾Œçš„å…§å®¹)
        # æ”¯æ´ * æˆ– - é–‹é ­ï¼Œæˆ–æ˜¯ç²—é«”
        if "è®“æˆ‘å€‘ä¿æŒè¯ç¹«" in line:
            break
            
        if not line.strip(): # ç©ºè¡Œ
            i += 1
            continue

        item = {'content': line, 'type': 'text'}
        
        # --- Table Detection ---
        # æª¢æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦ç‚ºåˆ†éš”ç·š (---|---|...)
        if i + 1 < len(lines):
            next_line = lines[i+1].strip()
            # åˆ†éš”ç·šç‰¹å¾µï¼šåŒ…å« |ï¼Œä¸”åªç”± - : | ç©ºç™½ çµ„æˆï¼Œä¸”è‡³å°‘æœ‰ä¸€å€‹ -
            if "|" in next_line and "-" in next_line and re.match(r'^[\s\-:|]+$', next_line):
                # ç™¼ç¾è¡¨æ ¼ï¼
                table_lines = [line] # Header
                table_lines.append(lines[i+1]) # Separator
                
                # ç¹¼çºŒå¾€ä¸‹æŠ“å–è¡¨æ ¼å…§å®¹
                k = i + 2
                while k < len(lines):
                    curr = lines[k].strip()
                    # ç°¡å–®åˆ¤æ–·ï¼šåªè¦é€™è¡Œæœ‰ | ä¸”ä¸æ˜¯ç©ºè¡Œï¼Œå°±è¦–ç‚ºè¡¨æ ¼çš„ä¸€éƒ¨åˆ†
                    # (é€™å¯èƒ½èª¤åˆ¤ï¼Œä½†åœ¨ä¸€èˆ¬ html2text è¼¸å‡ºä¸­é€šå¸¸æ˜¯é€£çºŒçš„)
                    if "|" in curr:
                         table_lines.append(lines[k])
                         k += 1
                    else:
                         break
                
                item['type'] = 'table'
                item['content'] = table_lines #List of strings
                parsed_items.append(item)
                i = k
                continue

        # 1. Header
        if line.startswith('#'):
            level = line.count('#')
            content = line.replace('#', '').strip()
            # ç§»é™¤ç²—é«”èˆ‡å‰ç¶´
            if content.startswith("**") and content.endswith("**"):
                content = content[2:-2].strip()
            content = clean_text_prefixes(content)
            
            if level < min_header_level:
                min_header_level = level
                
            item['type'] = 'header'
            item['level'] = level
            item['content'] = content
            parsed_items.append(item)
            i += 1
            continue
            
        # 2. Blockquote
        if line.startswith('>'):
            item['type'] = 'quote'
            item['content'] = line
            parsed_items.append(item)
            i += 1
            continue

        # 3. é è™•ç†ï¼šè§£æç¸®æ’èˆ‡ç§»é™¤åˆ—è¡¨ç¬¦è™Ÿ
        # è¨ˆç®—ä¾†æºç¸®æ’ (è§£æ±ºã€Œå‡å…§ç¸®ã€å•é¡Œ) - å‡è¨­ 2 ç©ºæ ¼ = 1 å±¤
        stripped_line = line.lstrip()
        indent_spaces = len(line) - len(stripped_line)
        indent_level = indent_spaces // 2 
        item['indent_level'] = indent_level

        clean_line = stripped_line
        is_list = False
        
        # åˆ¤æ–·æ˜¯å¦ç‚ºåˆ—è¡¨é …ç›® (æ”¯æ´ *, -, + å’Œ 1. 2. ç­‰)
        # ä½¿ç”¨ Regex åµæ¸¬ä¸¦ç§»é™¤ç¬¦è™Ÿï¼Œé¿å… Logseq å‡ºç¾ã€Œé›™é‡ bulletã€æˆ–ã€Œbullet + æ•¸å­—ã€
        match_ul = re.match(r'^[*+-]\s+(.*)', stripped_line)
        match_ol = re.match(r'^(\d+)\.\s+(.*)', stripped_line)

        if match_ul:
            clean_line = match_ul.group(1).strip()
            is_list = True
        elif match_ol:
            clean_line = match_ol.group(2).strip()
            is_list = True
        
        # ç‰¹æ®Šè™•ç†ï¼šè‹¥é–‹é ­æ˜¯åœ–ç‰‡æ ¼å¼ ![...](...)ï¼Œå‰‡ä¸æ‡‰è¢«åˆ‡æ–·
        # ä¸‹æ–¹ç¬¬ 4 æ­¥æœƒè™•ç† imageï¼Œé€™è£¡åªéœ€ç¢ºä¿ clean_line ä¹¾æ·¨å³å¯

        # 4. Image åˆ¤æ–· (åŒ…å« list å…§çš„ image)
        if clean_line.startswith('!['):
            # æª¢æŸ¥æ˜¯å¦æœ‰å¾ŒçºŒæ–‡å­—ä½œç‚º Image Caption
            match = re.match(r'^(!\[.*?\]\(.*?\))\s*(.*)', clean_line)
            if match:
                img_part = match.group(1)
                caption_part = match.group(2)
                
                item['type'] = 'image'
                item['content'] = img_part
                parsed_items.append(item)
                
                if caption_part:
                    caption_item = {
                        'type': 'caption',
                        'content': caption_part
                    }
                    parsed_items.append(caption_item)
            else:
                item['type'] = 'image'
                item['content'] = clean_line
                parsed_items.append(item)
            i += 1
            continue

        # 5. List Item or Paragraph
        item['content'] = clean_line
        if is_list:
             item['type'] = 'list_item'
        else:
             item['type'] = 'paragraph'
        
        parsed_items.append(item)
        i += 1

    # è¨ˆç®— Header ä½ç§»é‡ (è®“æœ€å° Header è®Šæˆ H2)
    if min_header_level == 999:
        header_offset = 0
    else:
        header_offset = 2 - min_header_level
    
    # é‡çµ„ Outlinerï¼Œè™•ç†ç¸®æ’é‚è¼¯
    outliner_lines = []
    
    current_header_indent = "" 
    list_indent_level = 0  # é¡å¤–çš„åˆ—è¡¨ç¸®æ’å±¤ç´š
    previous_item_type = None
    previous_content = ""
    
    # ç‹€æ…‹ï¼šæ˜¯å¦è™•æ–¼ã€Œå†’è™Ÿå¾Œçš„åˆ—è¡¨å€å¡Šã€
    in_colon_list_group = False
    
    for item in parsed_items:
        # 1. Header è™•ç†ï¼šé‡è¨­æ‰€æœ‰ç¸®æ’ç‹€æ…‹
        if item['type'] == 'header':
            in_colon_list_group = False
            list_indent_level = 0
            
            new_level = max(2, item['level'] + header_offset)
            indent_level = max(0, new_level - 2)
            current_header_indent = "  " * indent_level
            
            hashes = "#" * new_level
            outliner_lines.append(f"{current_header_indent}- {hashes} {item['content']}")
            
            # Header ä¸‹çš„å…§å®¹é è¨­ç¸®æ’ + 1
            current_body_indent = "  " * (indent_level + 1)
            
        # 2. Table è™•ç†
        elif item['type'] == 'table':
            in_colon_list_group = False # è¡¨æ ¼ä¸­æ–·åˆ—è¡¨
            final_indent = current_body_indent if 'current_body_indent' in locals() else ""
            
            # table_lines æ˜¯ä¸€å€‹ list
            table_lines = item['content']
            if table_lines:
                # ç¬¬ä¸€è¡Œå¸¶ bullet
                outliner_lines.append(f"{final_indent}- {table_lines[0]}")
                # å¾ŒçºŒè¡Œç¸®æ’å°é½Šæ–‡å­— (Enter in Logseq block)
                # ç¸®æ’ = indent + 2 spaces
                sub_indent = final_indent + "  "
                for t_line in table_lines[1:]:
                    outliner_lines.append(f"{sub_indent}{t_line}")

        # 3. åˆ—è¡¨é …ç›®è™•ç†
        elif item['type'] == 'list_item':
            # åˆ¤æ–·æ˜¯å¦é€²å…¥å†’è™Ÿå¾Œçš„åˆ—è¡¨ç¾¤çµ„
            if previous_item_type == 'paragraph' and (previous_content.endswith('ï¼š') or previous_content.endswith(':')):
                in_colon_list_group = True
            
            # å¦‚æœæ˜¯ Header ä¹‹å¾Œç›´æ¥æ¥åˆ—è¡¨ï¼Œé€šå¸¸è¦é‡è¨­ç¾¤çµ„
            if previous_item_type == 'header':
                in_colon_list_group = False

            # æ±ºå®šç¸®æ’
            final_indent = current_body_indent if 'current_body_indent' in locals() else ""
            
            # åŠ å…¥ä¾†æºç¸®æ’ (from indentation detection)
            source_indent = "  " * item.get('indent_level', 0)
            final_indent += source_indent

            if in_colon_list_group:
                final_indent += "  " # å¢åŠ ä¸€å±¤ç¸®æ’
            else:
                pass # ç¶­æŒæ¨™æº– body ç¸®æ’
                
            outliner_lines.append(f"{final_indent}- {item['content']}")
            
        # 4. Caption è™•ç†
        elif item['type'] == 'caption':
            # Caption ç¸®æ’æ¯” body å¤šä¸€å±¤
            final_indent = current_body_indent if 'current_body_indent' in locals() else ""
            outliner_lines.append(f"{final_indent}  - {item['content']}")

        # 5. å…¶ä»–å…§å®¹ (Paragraph, Quote, Image)
        else:
            # é‡åˆ°éåˆ—è¡¨é …ç›®ï¼Œé€šå¸¸æœƒä¸­æ–·åˆ—è¡¨ç¾¤çµ„
            if item['type'] == 'paragraph':
                 in_colon_list_group = False
            
            final_indent = current_body_indent if 'current_body_indent' in locals() else ""
            
            # åŠ å…¥ä¾†æºç¸®æ’
            source_indent = "  " * item.get('indent_level', 0)
            final_indent += source_indent

            if item['type'] == 'quote':
                 # è‹¥ä¸Šä¸€è¡Œæ˜¯ç¸®æ’åˆ—è¡¨ï¼Œå¼•ç”¨æ˜¯å¦è©²è·Ÿè‘—ç¸®ï¼Ÿ
                 # ç°¡å–®èµ·è¦‹ï¼Œå¼•ç”¨è·Ÿéš¨åŸºæœ¬ body ç¸®æ’ï¼Œå†åŠ ä¸€å±¤ Logseq å¼•ç”¨ç¸®æ’
                 outliner_lines.append(f"{final_indent}  - {item['content']}")
            else:
                 outliner_lines.append(f"{final_indent}- {item['content']}")
        
        previous_item_type = item['type']
        # content å¯èƒ½æ˜¯ list (table)ï¼Œè½‰å­—ä¸²ä»¥ä¾¿ check
        if isinstance(item['content'], list):
            previous_content = str(item['content'][0])
        else:
            previous_content = item['content']
            
    return '\n'.join(outliner_lines)

# æ–°å¢è¼”åŠ©å‡½å¼
def clean_text_prefixes(text):
    import re
    # å°æ‡‰ refine_notes.py çš„ç§»é™¤æ¸…å–®
    prefixes = [
        "ã€å…­å”å”¯ç‰©è§£ã€‘", "ã€simproã€‘", "simpro-", "Simpro-"
    ]
    
    # å˜—è©¦ URL decode
    try:
        import urllib.parse
        decoded = urllib.parse.unquote(text)
        if decoded != text:
             text = decoded
    except:
        pass

    for prefix in prefixes:
        if text.lower().startswith(prefix.lower()):
            text = re.sub(f"^{re.escape(prefix)}", "", text, flags=re.IGNORECASE).strip()
            
    return text.strip()

def fetch_posts():
    ensure_dirs()
    
    page = 1
    total_count = 0
    redirects = []  # æ”¶é›†é‡å®šå‘è¦å‰‡
    
    print(f"ğŸš€ é–‹å§‹å¾ {SITE_URL} æŠ“å–æ–‡ç«  (å«åœ–ç‰‡)...")

    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}

    while True:
        api_url = f"{SITE_URL}/wp-json/wp/v2/posts?page={page}&per_page={PER_PAGE}&_embed"
        
        # Retry mechanism
        max_retries = 3
        
        response = None
        for attempt in range(max_retries):
            try:
                print(f"  æ­£åœ¨è«‹æ±‚ API (é é¢ {page}, å˜—è©¦ {attempt+1}/{max_retries})...")
                response = requests.get(api_url, headers=headers, timeout=30)
                if response.status_code == 200:
                    break
                elif response.status_code in [400, 404]:
                    break
                else:
                    print(f"  âš ï¸  API å›å‚³é 200 ç‹€æ…‹: {response.status_code}")
            except Exception as e:
                print(f"  âš ï¸  é€£ç·šå˜—è©¦å¤±æ•—: {e}")
                if attempt < max_retries - 1:
                    import time
                    time.sleep(2)
        
        if not response:
            print("âŒ æ”¾æ£„æ­¤é é¢ï¼Œç„¡æ³•é€£ç·šã€‚")
            break

        if response.status_code == 400 or response.status_code == 404:
            print("âœ… å·²æŠµé”æœ€å¾Œä¸€é æˆ–ç„¡å…§å®¹ã€‚")
            break 
            
        if response.status_code != 200:
            print(f"âŒ éŒ¯èª¤: ç„¡æ³•é€£çµ API (Status: {response.status_code})")
            break

        try:
            posts = response.json()
        except:
            print("âŒ å›å‚³è³‡æ–™é JSON æ ¼å¼")
            break
            
        if not posts:
            print("âœ… ç„¡æ›´å¤šæ–‡ç« ã€‚")
            break

        for post in posts:
            raw_title = post['title']['rendered']
            title = clean_text_prefixes(raw_title) # æ¸…ç†æ¨™é¡Œ
            
            slug = post['slug']
            content_html = post['content']['rendered']
            date = post['date']
            link = post['link']
            
            print(f"æ­£åœ¨è™•ç†: {title}")

            # å–å¾—æ¨™ç±¤èˆ‡åˆ†é¡
            tags = []
            categories = []
            if '_embedded' in post and 'wp:term' in post['_embedded']:
                terms = post['_embedded']['wp:term']
                for term_group in terms:
                    for term in term_group:
                        if term['taxonomy'] == 'post_tag':
                            tags.append(term['name'])
                        elif term['taxonomy'] == 'category':
                            categories.append(term['name'])
            
            # å°‡åˆ†é¡åˆä½µè‡³æ¨™ç±¤ï¼Œä»¥ä¾¿åœ¨ Quartz æ¨™ç±¤é é¢é¡¯ç¤º
            # æ’é™¤ 'Uncategorized'
            for cat in categories:
                if cat != "Uncategorized":
                    tags.append(cat)

            # è½‰æ›å…§å®¹
            logseq_body = convert_to_outliner(content_html)
            
            # é è¨­ç‚ºå…¬é–‹ (draft: false)ï¼Œä½†è‹¥åˆ†é¡åŒ…å«ã€Œç§å¯†ã€ï¼Œå‰‡è¨­ç‚ºè‰ç¨¿ (draft: true) ä»¥ä¾¿ Quartz éæ¿¾
            is_draft = "false"
            for cat in categories:
                if "ç§å¯†" in cat:
                    is_draft = "true"
                    break

            # çµ„åˆ Logseq Page (æ‰å¹³çµæ§‹ï¼Œä½¿ç”¨ frontmatter æŒ‡å®šåˆ†é¡)
            # å‡ºç‰ˆæ™‚ç”± logseq_publish.py ä¾æ“š categories çµ„ç¹”åˆ°å­è³‡æ–™å¤¾
            header = f"""---
title: "{title}"
date: {date}
tags: {', '.join(tags)}
categories: {', '.join(categories)}
original_url: "{link}"
draft: {is_draft}
---

"""
            # è§£ç¢¼æª”åä¸¦æ¸…ç†å‰ç¶´
            try:
                import urllib.parse
                decoded_slug = urllib.parse.unquote(slug)
            except:
                decoded_slug = slug
            
            clean_slug = clean_text_prefixes(decoded_slug) # æ¸…ç†æª”å
            safe_filename = clean_slug.replace("/", "-").replace(":", "-")
            filename = f"{safe_filename}.md"
            
            # å¯«å…¥è‡³ Pages è³‡æ–™å¤¾ (æ‰å¹³çµæ§‹ï¼Œé…åˆ Logseq)
            target_path = os.path.join(OUTPUT_DIR, filename)
            abs_path = os.path.abspath(target_path)
            with open(target_path, 'w', encoding='utf-8') as f:
                f.write(header + logseq_body)
            
            total_count += 1
            print(f"âœ… [{total_count}] å·²å„²å­˜: {abs_path}")

        page += 1

    print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼ç¸½å…±è½‰æ› {total_count} ç¯‡æ–‡ç« ã€‚")
    
    # æ³¨æ„ï¼š_redirects æª”æ¡ˆç¾åœ¨ç”± logseq_publish.py ç”¢ç”Ÿ

if __name__ == "__main__":
    fetch_posts()